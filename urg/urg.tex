\documentclass[main]{subfiles}
\begin{document}
\chapter{座標変換したセンサデータをプロットする}

これは講義資料「第4回 オドメトリと座標系」内の課題です。

\section{課題概要}
測域センサをつけたフリー状態（外力を受けて動く状態）の
ロボットを動かしてマッピングする。

ロボットを中心とした「方向」と「距離」の情報をもつ
極座標系のデータをセンサから受け取り、
ロボットを原点(0, 0)\footnote{（X座標, Y座標）}
とする直交座標系のデータ（FS座標系データ）に変換するだけの
プログラムが与えられる。
これをスタート地点を原点(0, 0)とする平面に関して唯一の値をとる
直交座標系のデータ（GL座標系データ）に変換する処理を記述してマッピングを行う。

\section{測域センサの概要}
測域センサには北陽電機株式会社の「URG-04LX」を使用する。
ロボットの前方に取り付けられたセンサがPCとUSB接続され、
プロトコルSCIP2.0を利用して通信を行う。

\section{解法}
\begin{figure}[H]
	\begin{minipage}{0.5\hsize}
		\setlength{\parindent}{1\Cwd}
		FS座標系もGL座標系も同一平面上の点の位置を定めるものなので、
		FS座標系で表現される点をGL座標系に変換するには、
		\textbf{現在地のGL座標}とスタート地点において$\theta = 0$と定義する
		\textbf{ロボットの傾き$\theta$}(rad)が必要となる。

		具体的な処理を明確にするためにプログラムを設計する前に右のような図を描いた。
		目的は、FS座標(Px, Py)で表現される赤い点をGL座標(glPx, glPy)に変換することだ。
		右図においてXY軸が一般的な図からは90度ほど回転しているが、
		直進するとX座標の値が増え、左に進むとY座標の値が増える仕様に合わせたためだ。
	\end{minipage}
	\begin{minipage}{0.5\hsize}
		\begin{figure}[H]
			\centering
			\includegraphics[width=8cm]{img/trans.pdf}
			\caption{FS座標系とGL座標系の関係}
		\end{figure}
	\end{minipage}
\end{figure}

まず現在のロボットの傾き\textit{pos\_theta\_gl}を用いて物体の座標を回転させる。
座標の回転といっても同一平面上の点の回転移動と同じであるから、
以下のような行列計算で求めることができる。
\[
	\left(
	\begin{array}{cc}
		x \\
		y
	\end{array}
	\right)
	=
	\left(
	\begin{array}{cc}
		cos(pos\_theta\_gl) & -sin(pos\_theta\_gl) \\
		sin(pos\_theta\_gl) & cos(pos\_theta\_gl)
	\end{array}
	\right)
	\left(
	\begin{array}{c}
		x\_fs \\
		y\_fs
	\end{array}
	\right)
\]

上記の記述でロボットの傾き$\theta $が0のときと同じ座標系に移すことができたが、
これでは原点がロボット中心になりマップを作ることができない。
ここで現在地のGL座標(pos\_x\_gl, pos\_y\_gl)を加算する。
これはGL座標の原点からFS座標の原点（ロボット中心）への単純な平行移動で実現される。
先の行列計算に合わせて記述すると以下のようになる。
\[
	\left(
	\begin{array}{cc}
		x\_gl \\
		y\_gl
	\end{array}
	\right)
	=
	\left(
	\begin{array}{c}
		pos\_x\_gl \\
		pos\_y\_gl
	\end{array}
	\right)
	+
	\left(
	\begin{array}{cc}
		cos(pos\_theta\_gl) & -sin(pos\_theta\_gl) \\
		sin(pos\_theta\_gl) & cos(pos\_theta\_gl)
	\end{array}
	\right)
	\left(
	\begin{array}{c}
		x\_fs \\
		y\_fs
	\end{array}
	\right)
\]

こうして得られる物体の座標はGL座標の原点を基準とした平面で唯一の座標である。

\section{結果}
実際にフリー状態のロボットを動かしてマッピングしてみる。

\begin{figure}[H]
	\centering
	\begin{minipage}{0.2\hsize}
		\includegraphics[width=3cm]{img/photo_28.jpg}
		\caption{実際の場所A}
	\end{minipage}
	\begin{minipage}{0.28\hsize}
		\includegraphics[width=3cm]{img/28.png}
		\caption{作成したマップA}
	\end{minipage}
	\begin{minipage}{0.2\hsize}
		\includegraphics[width=3cm]{img/photo_ga.jpg}
		\caption{実際の場所B}
	\end{minipage}
	\begin{minipage}{0.29\hsize}
		\includegraphics[width=3cm]{img/ma.png}
		\caption{作成したマップB（y=6の壁は即席のもの）}
	\end{minipage}
\end{figure}

ノイズが目立つが正しくマッピングできている。

\section{考察}
座標系の変換がセンサをより強力なツールに変えた。
URGから送られるデータはロボットから
壁（単に光を反射するもの）までのベクトルに過ぎないが、
ロボットの位置情報を使った座標系の変換を行うことで、
世界中の壁を記録することができる。

第6章の最終課題では、これをさらに物体認識を行うツールへと強化する。
これにより単なる壁の記録のみならず、
同一物体を検出しオブジェクトとして記録できるようになる。

\end{document}
